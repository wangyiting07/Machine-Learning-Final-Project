{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish processing data\n",
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=10, bias=True)\n",
      ")\n",
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 11.181166\n",
      "Train Epoch: 0 [2000/50000 (4%)]\tLoss: 1.844738\n",
      "Train Epoch: 0 [4000/50000 (8%)]\tLoss: 0.825811\n",
      "Train Epoch: 0 [6000/50000 (12%)]\tLoss: 0.569415\n",
      "Train Epoch: 0 [8000/50000 (16%)]\tLoss: 0.575305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:88: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [10000/50000 (20%)]\tLoss: 0.407443\n",
      "Train Epoch: 0 [12000/50000 (24%)]\tLoss: 0.311249\n",
      "Train Epoch: 0 [14000/50000 (28%)]\tLoss: 0.415420\n",
      "Train Epoch: 0 [16000/50000 (32%)]\tLoss: 0.291766\n",
      "Train Epoch: 0 [18000/50000 (36%)]\tLoss: 0.267830\n",
      "Train Epoch: 0 [20000/50000 (40%)]\tLoss: 0.396557\n",
      "Train Epoch: 0 [22000/50000 (44%)]\tLoss: 0.238669\n",
      "Train Epoch: 0 [24000/50000 (48%)]\tLoss: 0.399167\n",
      "Train Epoch: 0 [26000/50000 (52%)]\tLoss: 0.284682\n",
      "Train Epoch: 0 [28000/50000 (56%)]\tLoss: 0.286793\n",
      "Train Epoch: 0 [30000/50000 (60%)]\tLoss: 0.256031\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 0.290577\n",
      "Train Epoch: 0 [34000/50000 (68%)]\tLoss: 0.205535\n",
      "Train Epoch: 0 [36000/50000 (72%)]\tLoss: 0.183592\n",
      "Train Epoch: 0 [38000/50000 (76%)]\tLoss: 0.260386\n",
      "Train Epoch: 0 [40000/50000 (80%)]\tLoss: 0.331396\n",
      "Train Epoch: 0 [42000/50000 (84%)]\tLoss: 0.208044\n",
      "Train Epoch: 0 [44000/50000 (88%)]\tLoss: 0.196137\n",
      "Train Epoch: 0 [46000/50000 (92%)]\tLoss: 0.256663\n",
      "Train Epoch: 0 [48000/50000 (96%)]\tLoss: 0.224071\n",
      "finish training\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.202503\n",
      "Train Epoch: 1 [2000/50000 (4%)]\tLoss: 0.146240\n",
      "Train Epoch: 1 [4000/50000 (8%)]\tLoss: 0.181740\n",
      "Train Epoch: 1 [6000/50000 (12%)]\tLoss: 0.172976\n",
      "Train Epoch: 1 [8000/50000 (16%)]\tLoss: 0.183145\n",
      "Train Epoch: 1 [10000/50000 (20%)]\tLoss: 0.172593\n",
      "Train Epoch: 1 [12000/50000 (24%)]\tLoss: 0.155164\n",
      "Train Epoch: 1 [14000/50000 (28%)]\tLoss: 0.184212\n",
      "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 0.106911\n",
      "Train Epoch: 1 [18000/50000 (36%)]\tLoss: 0.128337\n",
      "Train Epoch: 1 [20000/50000 (40%)]\tLoss: 0.274648\n",
      "Train Epoch: 1 [22000/50000 (44%)]\tLoss: 0.226357\n",
      "Train Epoch: 1 [24000/50000 (48%)]\tLoss: 0.216539\n",
      "Train Epoch: 1 [26000/50000 (52%)]\tLoss: 0.189289\n",
      "Train Epoch: 1 [28000/50000 (56%)]\tLoss: 0.153212\n",
      "Train Epoch: 1 [30000/50000 (60%)]\tLoss: 0.104608\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.116924\n",
      "Train Epoch: 1 [34000/50000 (68%)]\tLoss: 0.125429\n",
      "Train Epoch: 1 [36000/50000 (72%)]\tLoss: 0.146546\n",
      "Train Epoch: 1 [38000/50000 (76%)]\tLoss: 0.138682\n",
      "Train Epoch: 1 [40000/50000 (80%)]\tLoss: 0.118429\n",
      "Train Epoch: 1 [42000/50000 (84%)]\tLoss: 0.176454\n",
      "Train Epoch: 1 [44000/50000 (88%)]\tLoss: 0.189114\n",
      "Train Epoch: 1 [46000/50000 (92%)]\tLoss: 0.130727\n",
      "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 0.213094\n",
      "finish training\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.137543\n",
      "Train Epoch: 2 [2000/50000 (4%)]\tLoss: 0.121348\n",
      "Train Epoch: 2 [4000/50000 (8%)]\tLoss: 0.055665\n",
      "Train Epoch: 2 [6000/50000 (12%)]\tLoss: 0.107271\n",
      "Train Epoch: 2 [8000/50000 (16%)]\tLoss: 0.059175\n",
      "Train Epoch: 2 [10000/50000 (20%)]\tLoss: 0.133404\n",
      "Train Epoch: 2 [12000/50000 (24%)]\tLoss: 0.125169\n",
      "Train Epoch: 2 [14000/50000 (28%)]\tLoss: 0.083291\n",
      "Train Epoch: 2 [16000/50000 (32%)]\tLoss: 0.130462\n",
      "Train Epoch: 2 [18000/50000 (36%)]\tLoss: 0.050174\n",
      "Train Epoch: 2 [20000/50000 (40%)]\tLoss: 0.083062\n",
      "Train Epoch: 2 [22000/50000 (44%)]\tLoss: 0.066776\n",
      "Train Epoch: 2 [24000/50000 (48%)]\tLoss: 0.096014\n",
      "Train Epoch: 2 [26000/50000 (52%)]\tLoss: 0.059120\n",
      "Train Epoch: 2 [28000/50000 (56%)]\tLoss: 0.061713\n",
      "Train Epoch: 2 [30000/50000 (60%)]\tLoss: 0.053241\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.050046\n",
      "Train Epoch: 2 [34000/50000 (68%)]\tLoss: 0.135691\n",
      "Train Epoch: 2 [36000/50000 (72%)]\tLoss: 0.077387\n",
      "Train Epoch: 2 [38000/50000 (76%)]\tLoss: 0.102448\n",
      "Train Epoch: 2 [40000/50000 (80%)]\tLoss: 0.143489\n",
      "Train Epoch: 2 [42000/50000 (84%)]\tLoss: 0.088487\n",
      "Train Epoch: 2 [44000/50000 (88%)]\tLoss: 0.164458\n",
      "Train Epoch: 2 [46000/50000 (92%)]\tLoss: 0.186079\n",
      "Train Epoch: 2 [48000/50000 (96%)]\tLoss: 0.092463\n",
      "finish training\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.087607\n",
      "Train Epoch: 3 [2000/50000 (4%)]\tLoss: 0.104552\n",
      "Train Epoch: 3 [4000/50000 (8%)]\tLoss: 0.119541\n",
      "Train Epoch: 3 [6000/50000 (12%)]\tLoss: 0.136281\n",
      "Train Epoch: 3 [8000/50000 (16%)]\tLoss: 0.093247\n",
      "Train Epoch: 3 [10000/50000 (20%)]\tLoss: 0.113210\n",
      "Train Epoch: 3 [12000/50000 (24%)]\tLoss: 0.069770\n",
      "Train Epoch: 3 [14000/50000 (28%)]\tLoss: 0.150104\n",
      "Train Epoch: 3 [16000/50000 (32%)]\tLoss: 0.066491\n",
      "Train Epoch: 3 [18000/50000 (36%)]\tLoss: 0.095372\n",
      "Train Epoch: 3 [20000/50000 (40%)]\tLoss: 0.080271\n",
      "Train Epoch: 3 [22000/50000 (44%)]\tLoss: 0.060512\n",
      "Train Epoch: 3 [24000/50000 (48%)]\tLoss: 0.137690\n",
      "Train Epoch: 3 [26000/50000 (52%)]\tLoss: 0.079719\n",
      "Train Epoch: 3 [28000/50000 (56%)]\tLoss: 0.065543\n",
      "Train Epoch: 3 [30000/50000 (60%)]\tLoss: 0.064775\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.121978\n",
      "Train Epoch: 3 [34000/50000 (68%)]\tLoss: 0.084316\n",
      "Train Epoch: 3 [36000/50000 (72%)]\tLoss: 0.098341\n",
      "Train Epoch: 3 [38000/50000 (76%)]\tLoss: 0.155461\n",
      "Train Epoch: 3 [40000/50000 (80%)]\tLoss: 0.161297\n",
      "Train Epoch: 3 [42000/50000 (84%)]\tLoss: 0.108728\n",
      "Train Epoch: 3 [44000/50000 (88%)]\tLoss: 0.054083\n",
      "Train Epoch: 3 [46000/50000 (92%)]\tLoss: 0.112988\n",
      "Train Epoch: 3 [48000/50000 (96%)]\tLoss: 0.102601\n",
      "finish training\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.052322\n",
      "Train Epoch: 4 [2000/50000 (4%)]\tLoss: 0.043460\n",
      "Train Epoch: 4 [4000/50000 (8%)]\tLoss: 0.074762\n",
      "Train Epoch: 4 [6000/50000 (12%)]\tLoss: 0.112380\n",
      "Train Epoch: 4 [8000/50000 (16%)]\tLoss: 0.098470\n",
      "Train Epoch: 4 [10000/50000 (20%)]\tLoss: 0.076612\n",
      "Train Epoch: 4 [12000/50000 (24%)]\tLoss: 0.082985\n",
      "Train Epoch: 4 [14000/50000 (28%)]\tLoss: 0.045877\n",
      "Train Epoch: 4 [16000/50000 (32%)]\tLoss: 0.058106\n",
      "Train Epoch: 4 [18000/50000 (36%)]\tLoss: 0.077127\n",
      "Train Epoch: 4 [20000/50000 (40%)]\tLoss: 0.055697\n",
      "Train Epoch: 4 [22000/50000 (44%)]\tLoss: 0.062271\n",
      "Train Epoch: 4 [24000/50000 (48%)]\tLoss: 0.042263\n",
      "Train Epoch: 4 [26000/50000 (52%)]\tLoss: 0.082205\n",
      "Train Epoch: 4 [28000/50000 (56%)]\tLoss: 0.121985\n",
      "Train Epoch: 4 [30000/50000 (60%)]\tLoss: 0.111064\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-ee963f75b118>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mrun_opt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mcreate_nn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-ee963f75b118>\u001b[0m in \u001b[0;36mcreate_nn\u001b[1;34m(batch_size, learning_rate, epochs, log_interval)\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m                 print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\sgd.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     92\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                         \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'momentum_buffer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                         \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import torch.utils.data as utils\n",
    "import csv\n",
    "\n",
    "predict = []\n",
    "data_path = \"./data.mat\"\n",
    "data_raw = loadmat(data_path)\n",
    "\n",
    "train_img = data_raw[\"train_img\"]\n",
    "test_img = data_raw[\"test_img\"]\n",
    "train_lbl = data_raw[\"train_lbl\"]\n",
    "\n",
    "#train_img = train_img.astype(np.float64)\n",
    "# train_img -= np.mean(train_img, axis = 0)\n",
    "# train_img /= np.std(train_img, axis = 0)\n",
    "# Max = [None]*784\n",
    "# for i in range(0,784):\n",
    "#     Max[i] = max(train_img[:,i])\n",
    "# train_img = train_img/Max\n",
    "\n",
    "#process training data \n",
    "tensor_train_img = torch.from_numpy(train_img) # transform to torch tensors\n",
    "tensor_train_lbl = torch.from_numpy(train_lbl)\n",
    "tensor_train_img = tensor_train_img.type(torch.FloatTensor)\n",
    "tensor_train_lbl = tensor_train_lbl.type(torch.LongTensor)\n",
    "# transform = transforms.Compose([\n",
    "#                        transforms.ToPILImage(),\n",
    "#                        transforms.ToTensor(),\n",
    "#                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                    ])\n",
    "my_trainset = utils.TensorDataset(tensor_train_img,tensor_train_lbl) # create your datset\n",
    "\n",
    "#process testing data\n",
    "test_lbl = np.zeros((20000))\n",
    "tensor_test_img = torch.from_numpy(test_img)\n",
    "tensor_test_lbl = torch.from_numpy(test_lbl)\n",
    "tensor_test_img = tensor_test_img.type(torch.FloatTensor)\n",
    "tensor_test_lbl = tensor_test_lbl.type(torch.LongTensor)\n",
    "my_testset = utils.TensorDataset(tensor_test_img,tensor_test_lbl)\n",
    "print(\"finish processing data\")\n",
    "\n",
    "def simple_gradient():\n",
    "    # print the gradient of 2x^2 + 5x\n",
    "    x = Variable(torch.ones(2, 2) * 2, requires_grad=True)\n",
    "    z = 2 * (x * x) + 5 * x\n",
    "    # run the backpropagation\n",
    "    z.backward(torch.ones(2, 2))\n",
    "    print(x.grad)\n",
    "\n",
    "\n",
    "def create_nn(batch_size=200, learning_rate=0.001, epochs=10,\n",
    "              log_interval=10):\n",
    "    \n",
    "    my_train_dataloader = utils.DataLoader(my_trainset,batch_size=batch_size, shuffle=True) # create your dataloader\n",
    "    my_test_dataloader = utils.DataLoader(my_testset,batch_size=batch_size, shuffle=True) # create your dataloader\n",
    "#     train_loader = torch.utils.data.DataLoader(\n",
    "#         datasets.MNIST('../data', train=True, download=False,\n",
    "#                        transform=transforms.Compose([\n",
    "#                            transforms.ToTensor(),\n",
    "#                            transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                        ])),\n",
    "#         batch_size=batch_size, shuffle=True)\n",
    "#     test_loader = torch.utils.data.DataLoader(\n",
    "#         datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.1307,), (0.3081,))\n",
    "#         ])),\n",
    "#         batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(28 * 28, 200)\n",
    "            self.fc2 = nn.Linear(200, 200)\n",
    "            self.fc3 = nn.Linear(200, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return F.log_softmax(x)\n",
    "\n",
    "    net = Net()\n",
    "    print(net)\n",
    "\n",
    "    # create a stochastic gradient descent optimizer\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    # create a loss function\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # run the main training loop\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(my_train_dataloader):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)\n",
    "            data = data.view(-1, 28*28)\n",
    "            target = target.view(200)\n",
    "            optimizer.zero_grad()\n",
    "            net_out = net(data)\n",
    "#             print(\"out: \",net_out)\n",
    "#             print(\"target: \",target)\n",
    "            loss = criterion(net_out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             if batch_idx % log_interval == 0:\n",
    "#                 print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                     epoch, batch_idx * len(data), len(my_train_dataloader.dataset),\n",
    "#                            100. * batch_idx / len(my_train_dataloader), loss.data[0]))\n",
    "#         print(\"finish training\")\n",
    "    # predict the test set label\n",
    "\n",
    "   \n",
    "    for data in tensor_test_img:\n",
    "        data = Variable(data, volatile=True)\n",
    "        data = data.view(-1, 28 * 28)\n",
    "        net_out = net(data)\n",
    "        # sum up batch loss\n",
    "        pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "        predict.append(pred[0])\n",
    "\n",
    "    print(\"finish testing\")\n",
    "    print(\"prediction: \", predict)\n",
    "\n",
    "            \n",
    "    print(\"finish writing\") \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_opt = 2\n",
    "    if run_opt == 1:\n",
    "        simple_gradient()\n",
    "    elif run_opt == 2:\n",
    "\n",
    "        create_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = np.arange(1,20001)\n",
    "ID = ID.tolist()\n",
    "data = zip(ID,predict)\n",
    "with open('first.csv', 'w',newline='') as outfile:\n",
    "    mywriter = csv.writer(outfile)\n",
    "    # manually add header\n",
    "\n",
    "    mywriter.writerow(['ID', 'Prediction'])\n",
    "    for d in data:\n",
    "        mywriter.writerow(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
